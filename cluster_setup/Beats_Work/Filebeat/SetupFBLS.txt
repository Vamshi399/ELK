#Configure filebeat (plugin) to send log lines to logstash

The Filebeat client is a lightweight, resource-friendly tool that collects logs from files on 
the server and forwards these logs to your Logstash instance for processing.
Filebeat is designed for reliability and low latency.Filebeat has a light resource footprint
on the host machine, and the Beats input plugin minimizes the resource demands on the Logstash
instance.

SETUP

#If UBUNTU
#Download and install the Public Signing Key:
$sudo wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -

#Install “apt-transport-https” and add repo.
$sudo apt-get install apt-transport-https

$sudo echo "deb https://artifacts.elastic.co/packages/6.x/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-6.x.list

#Update repo and install Filebeat
$sudo apt-get update
$sudo apt-get install filebeat

#If Centos
#to download and install the Elasticsearch public signing key:
sudo rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch

#add the Elastic repository
sudo vi /etc/yum.repos.d/elasticsearch.repo

[elasticsearch-6.x]
name=Elasticsearch repository for 6.x packages
baseurl=https://artifacts.elastic.co/packages/6.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md

then
yum install filebeat

CONFIGURATION

#Option 1: Here index name would be default "filebeat-%{[agent.version]}-%{+yyyy.MM.dd}"

#Sending data directly to ES
#Modify Filebeat configurations.
$sudo vim /etc/filebeat/filebeat.yml

enabled: true
paths:
    - /var/log/messages*

#comment the following lines:
#output.logstash:
  # The Logstash hosts
  #hosts: ["elk-server:5443"]
  #ssl.certificate_authorities: ["/etc/filebeat/logstash-forwarder.crt"]

#Uncomment Elasticsearch:
#output.elasticsearch:
  # Array of hosts to connect to.
  hosts: ["ce1:9200"]

check if index was created..

#Option 2: 
#Modify Filebeat configurations.
$sudo vim /etc/filebeat/filebeat.yml

enabled: true
paths:
    - /usr/local/elasticsearch/logs/mycluster.log

#comment the following lines:
#output.logstash:
  # The Logstash hosts
  #hosts: ["elk-server:5443"]
  #ssl.certificate_authorities: ["/etc/filebeat/logstash-forwarder.crt"]

#set the index dynamically by using a format string to access any event field
#The events are distributed to these nodes in round robin order. If one node becomes unreachable, the event is automatically sent to another node. 
#Uncomment Elasticsearch:
#output.elasticsearch:
  # Array of hosts to connect to.
  hosts: ["ce1:9200","ce2:9200","ce3:9200"]
  #if data contains field type:log_type ,that could be used to route data into 2 different indexes
  #With this configuration, all events with log_type: normal are sent to an index named normal-xxx and all events with log_type: critical are sent to critical-xxx
  #index: "%{[fields.log_type]}-%{[agent.version]}-%{+yyyy.MM.dd}"
  #setting our own index name, which would also mean if we need to use our own template 
  # "ESLog-%{[agent.version]}-%{+yyyy.MM.dd}"
  #setup.template.name: "ESLog"
  #setup.template.pattern: "ESLog-*"

#Addtional options ( which we will check later)
output.elasticsearch:
  hosts: ["http://localhost:9200"]
  indices:
    - index: "warning-%{[agent.version]}-%{+yyyy.MM.dd}"
      when.contains:
        message: "WARN"
    - index: "error-%{[agent.version]}-%{+yyyy.MM.dd}"
      when.contains:
        message: "ERR"

#Additional options (which we will check later)
output.elasticsearch:
  hosts: ["http://localhost:9200"]
  indices:
    - index: "%{[fields.log_type]}"
      mappings:
        critical: "sev1"
        normal: "sev2"
      default: "sev3"
 
check if index was created..

===========================================
#Sending data to logstash to fwd to ES
#Modify Filebeat configurations.
$sudo vim /etc/filebeat/filebeat.yml

enabled: true
paths:
    - /var/log/elasticsearch/mycluster.log

#Uncomment the following lines:
output.logstash:
  # The Logstash hosts
  hosts: ["ce4:5443"]
  ssl.certificate_authorities: ["/etc/filebeat/logstash-forwarder.crt"]

#Comment Elasticsearch:
#output.elasticsearch:
  # Array of hosts to connect to.
  # hosts: ["localhost:9200"]

#Now go to logstash server and get “logstash-forwarder.crt” contents
$sudo cat /etc/logstash/ssl/logstash-forwarder.crt
#get the contents 

#create a certificate file on filebeat client server
$sudo vim /etc/filebeat/logstash-forwarder.crt
#copy the logstash-forwarder.crt contents here

#Enable filebeat on system boot Start filebeat service
$sudo systemctl enable filebeat.service
$sudo systemctl start filebeat.service
$sudo service filebeat status

#we will start it again after we setup logstash to accept beats..

###on logstash running node#####
#Creating a sample pipeline on logstash node to accept beats from filebeat on client server
#To Test

$root@u1:~# vi /etc/logstash/conf.d/fb_pipeline.conf
input{
  beats {
        port => "5443"
        }
     }
output {
       stdout{codec => "rubydebug"}
       }

#Testing newly created config
[elk@ce4 ~]$ /usr/share/logstash/bin/logstash --path.settings /etc/logstash/ -f /etc/logstash/conf.d/fb_pipeline.conf --config.test_and_exit
Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[2020-11-26T22:31:44,420][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
Configuration OK
[2020-11-26T22:31:53,406][INFO ][logstash.runner          ] Using config.test_and_exit mode. Config Validation Result: OK. Exiting Logstash

#To run logstash with above mentioned config, remember to update output location in *.conf to receive content sent by filebeat and forward it 
#as per location

--refer input-filter-output content from fb_pipeline.conf

#now start logstash with this config
[elk@ce4 ~]$ /usr/share/logstash/bin/logstash --path.settings /etc/logstash/ -f /etc/logstash/conf.d/fb_pipeline.conf

###on filebeat running node####
#start filebeat service or start command to publish data 
root@u2:/usr/share/filebeat/bin# ./filebeat -e -c /etc/filebeat/filebeat.yml -d "publish"

#if fine, start filebeat service
#start filebeat as a service or start filebeat from command line from /usr/share/bin/filebeat
===========================
#check from Kibana if filebeat is sending data to logstash,if logstash is fwding same to Elasticsearch 
and a new index is created.

