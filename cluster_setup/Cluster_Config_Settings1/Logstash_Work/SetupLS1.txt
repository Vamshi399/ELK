#Logstash setup
#can be setup using tar or using RPM
The Elastic Stack components are not available through the package manager by default, but you can install them with yum by adding Elastic’s package repository.
All of the Elastic Stack’s packages are signed with the Elasticsearch signing key in order to protect your system from package spoofing. Packages which have been authenticated using the key will be considered trusted by your package manager. In this step, you will import the Elasticsearch public GPG key and add the Elastic repository in order to install Elasticsearch.

#to download and install the Elasticsearch public signing key:
sudo rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch

#add the Elastic repository
sudo vi /etc/yum.repos.d/elasticsearch.repo

[elasticsearch-6.x]
name=Elasticsearch repository for 6.x packages
baseurl=https://artifacts.elastic.co/packages/6.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md

#if installing ES using this method and if not already done
sudo yum install elasticsearch

#& then editing sudo vi /etc/elasticsearch/elasticsearch.yml

#if installing Kibana using this method and if not already done
sudo yum install kibana

LOGSTASH SETUP:
#To install logstash
sudo yum install logstash

#using openssl to create certificates.

$cd /etc/logstash
$sudo mkdir ssl
#Generate SSL certificate. Change value of CN to your es-server name in the below command.(for example: ce1 since ES is running on ce1 as being one of the nodes of ES cluster)
$sudo openssl req -subj '/CN=ce1/' -x509 -days 3650 -batch -nodes -newkey rsa:2048 -keyout ssl/logstash-forwarder.key -out ssl/logstash-forwarder.crt

#the files described below can be found in 'myconfigs/conf.d/'
#Create following files inside “/etc/logstash/conf.d”


#if configuring logstash to receive data sent by a beat
#sudo vim filebeat-input.conf
Add the following lines to it to configure input

input {
  beats {
    port => 5443
    type => syslog
    ssl => true
    ssl_certificate => "/etc/logstash/ssl/logstash-forwarder.crt"
    ssl_key => "/etc/logstash/ssl/logstash-forwarder.key"
  }
}
#Save and close the file and create a new configuration file

$sudo vim syslog-filter.conf
Add the following contents to it(here we are using GROK plugin) to filter/transform data

filter {
  if [type] == "syslog" {
    grok {
      match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
      add_field => [ "received_at", "%{@timestamp}" ]
      add_field => [ "received_from", "%{host}" ]
    }
    date {
      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
    }
  }
}
#Save and exit the file. Create elasticsearch output file.

$sudo vim output-elasticsearch.conf
Add the following lines to it to route output

output {
  elasticsearch { hosts => ["ce1:9200"]
    hosts => "ce1:9200"
    manage_template => false
    index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
    document_type => "%{[@metadata][type]}"
  }
}


#if starting as a non-root user
#remember to
sudo chown -R elk:elk /var/lib/logstash
sudo chown -R elk:elk /var/log/logstash

#by starting logstash in this way, we will have logstash ready to take input from a beat,filter it and push it to elasticsearch,as per our conf files in '/etc/logstash/conf.d'
/usr/share/logstash/bin/logstash --path.settings /etc/logstash

#look for following output
[elk@ce3 ~]$ /usr/share/logstash/bin/logstash --path.settings /etc/logstash
Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[2020-11-16T22:27:48,023][INFO ][logstash.setting.writabledirectory] Creating directory {:setting=>"path.queue", :path=>"/var/lib/logstash/queue"}
[2020-11-16T22:27:48,043][INFO ][logstash.setting.writabledirectory] Creating directory {:setting=>"path.dead_letter_queue", :path=>"/var/lib/logstash/dead_letter_queue"}
[2020-11-16T22:27:48,536][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"6.8.13"}
[2020-11-16T22:27:48,567][INFO ][logstash.agent           ] No persistent UUID file found. Generating new UUID {:uuid=>"0cfdeb0b-e792-46f7-9dc3-9d26b4339a38", :path=>"/var/lib/logstash/uuid"}
[2020-11-16T22:27:59,067][WARN ][logstash.outputs.elasticsearch] You are using a deprecated config setting "document_type" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Document types are being deprecated in Elasticsearch 6.0, and removed entirely in 7.0. You should avoid this feature If you have any questions about this, please visit the #logstash channel on freenode irc. {:name=>"document_type", :plugin=><LogStash::Outputs::ElasticSearch index=>"%{[@metadata][beat]}-%{+YYYY.MM.dd}", manage_template=>false, id=>"466f76dffb11c709b3cdaad13da660285839f9cfedc766a38f82492059568c3d", hosts=>[//ce1:9200], document_type=>"%{[@metadata][type]}", enable_metric=>true, codec=><LogStash::Codecs::Plain id=>"plain_0334f39e-ebe5-440d-bfee-ceaab0e1e332", enable_metric=>true, charset=>"UTF-8">, workers=>1, template_name=>"logstash", template_overwrite=>false, doc_as_upsert=>false, script_type=>"inline", script_lang=>"painless", script_var_name=>"event", scripted_upsert=>false, retry_initial_interval=>2, retry_max_interval=>64, retry_on_conflict=>1, ilm_enabled=>false, ilm_rollover_alias=>"logstash", ilm_pattern=>"{now/d}-000001", ilm_policy=>"logstash-policy", action=>"index", ssl_certificate_verification=>true, sniffing=>false, sniffing_delay=>5, timeout=>60, pool_max=>1000, pool_max_per_route=>100, resurrect_delay=>5, validate_after_inactivity=>10000, http_compression=>false>}
[2020-11-16T22:27:59,398][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>2, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50}
[2020-11-16T22:28:00,587][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://ce1:9200/]}}
[2020-11-16T22:28:00,825][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>"http://ce1:9200/"}
[2020-11-16T22:28:00,910][INFO ][logstash.outputs.elasticsearch] ES Output version determined {:es_version=>6}
[2020-11-16T22:28:00,913][WARN ][logstash.outputs.elasticsearch] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>6}
[2020-11-16T22:28:00,966][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//ce1:9200"]}
[2020-11-16T22:28:01,608][INFO ][logstash.inputs.beats    ] Beats inputs: Starting input listener {:address=>"0.0.0.0:5443"}
[2020-11-16T22:28:01,651][INFO ][logstash.pipeline        ] Pipeline started successfully {:pipeline_id=>"main", :thread=>"#<Thread:0x7e9701e6 run>"}
[2020-11-16T22:28:01,729][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2020-11-16T22:28:01,798][INFO ][org.logstash.beats.Server] Starting server on port: 5443
[2020-11-16T22:28:02,463][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}

-----------------------------
