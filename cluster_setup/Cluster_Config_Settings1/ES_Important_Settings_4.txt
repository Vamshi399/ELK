ES Important Settings
-------------------

Important to consider:

Disable swapping
Increase file descriptors
Ensure sufficient virtual memory
Ensure sufficient threads
JVM DNS cache settings
Temporary directory not mounted with noexec

------------
Disable swapping:
Swapping is very bad for performance, for node stability, and should be avoided at all costs. 
It can cause garbage collections to last for minutes instead of milliseconds and can cause nodes to respond slowly 
or even to disconnect from the cluster. 
In a resilient distributed system, it’s more effective to let the operating system kill the node.

three approaches to disabling swapping

usually ES is the only service running on a box, and its memory usage is controlled by the JVM options. 
There should be no need to have swap enabled.

sudo swapoff -a

#no restart required
To disable it permanently, you will need to edit the /etc/fstab file and comment out any lines that contain the word swap.

or

configure swappiness to 1
vm.swappiness is set to 1. This reduces the kernel’s tendency to swap and should not lead to 
swapping under normal circumstances, while still allowing the whole system to swap in emergency conditions.

or

Enable bootstrap.memory_lock

Another option is to use mlockall on Linux/Unix systems, or VirtualLock on Windows, to try to lock the process 
address space into RAM, preventing any Elasticsearch memory from being swapped out. 

This can be done, by adding this line to the config/elasticsearch.yml file:
bootstrap.memory_lock: true

verify:
GET _nodes?filter_path=**.mlockall

If you see that mlockall is false, then it means that the mlockall request has failed. 
You will also see a line with more information in the logs with the words Unable to lock JVM Memory.

The most probable reason, on Linux/Unix systems, is that the user running Elasticsearch doesn’t have permission 
to lock memory. This can be granted as follows:

.zip and .tar.gz
Set ulimit -l unlimited as root before starting Elasticsearch, 
or set memlock to unlimited in /etc/security/limits.conf.

RPM and Debian
Set MAX_LOCKED_MEMORY to unlimited in the system configuration file (or see below for systems using systemd).

Systems using systemd
Set LimitMEMLOCK to infinity in the systemd configuration.
Another possible reason why mlockall can fail is that the JNA temporary directory 
(usually a sub-directory of /tmp) is mounted with the noexec option. 
This can be solved by specifying a new temporary directory for JNA using the ES_JAVA_OPTS environment variable:

export ES_JAVA_OPTS="$ES_JAVA_OPTS -Djna.tmpdir=<path>"
./bin/elasticsearch

--------------
File Descriptors:

Elasticsearch uses a lot of file descriptors or file handles. Running out of file descriptors 
can be disastrous and will most probably lead to data loss. 
Make sure to increase the limit on the number of open files descriptors for the user 
running Elasticsearch to 65,536 or higher.

For the .zip and .tar.gz packages, set ulimit -n 65535 as root before starting Elasticsearch, or set nofile to 65535 in /etc/security/limits.conf.

On macOS, you must also pass the JVM option -XX:-MaxFDLimit to Elasticsearch in order for it to make use of the higher file descriptor limit.

RPM and Debian packages already default the maximum number of file descriptors to 65535 and do not require further configuration.

You can check the max_file_descriptors configured for each node using the Nodes stats API, with:
GET _nodes/stats/process?filter_path=**.max_file_descriptors

------------
Virtual memory:
Elasticsearch uses a mmapfs directory by default to store its indices. 
The default operating system limits on mmap counts is likely to be too low, 
which may result in out of memory exceptions.

sysctl -w vm.max_map_count=262144

To set this value permanently, update the vm.max_map_count setting in /etc/sysctl.conf. 
To verify after rebooting, run sysctl vm.max_map_count.

---------------
No of threads:

Elasticsearch uses a number of thread pools for different types of operations. 
It is important that it is able to create new threads whenever needed. 
Make sure that the number of threads that the Elasticsearch user can create is at least 4096.

This can be done by setting 
ulimit -u 4096 as root 
before starting Elasticsearch, 

or by setting nproc to 4096 in /etc/security/limits.conf.

-----------------
DNS cache settings

Elasticsearch runs with a security manager in place. 
With a security manager in place, the JVM defaults to caching positive hostname resolutions indefinitely 
and defaults to caching negative hostname resolutions for ten seconds. 

Elasticsearch overrides this behavior with default values to cache positive lookups for sixty seconds, 
and to cache negative lookups for ten seconds. 

These values should be suitable for most environments, including environments where DNS resolutions vary with time. 
If not, you can edit the values 
es.networkaddress.cache.ttl 
and es.networkaddress.cache.negative.ttl in the JVM options. 

Note that the values networkaddress.cache.ttl=<timeout> and networkaddress.cache.negative.ttl=<timeout> 
in the Java security policy are ignored by Elasticsearch unless you remove the settings for 
es.networkaddress.cache.ttl and es.networkaddress.cache.negative.ttl.

-----------------
JNA temporary directory not mounted with noexec (Applicable to linux only)

Elasticsearch uses the Java Native Access (JNA) library for executing some platform-dependent native code. 
On Linux, the native code backing this library is extracted at runtime from the JNA archive. 
By default, this code is extracted to the Elasticsearch temporary directory which defaults to a sub-directory of /tmp. 

Alternatively, this location can be controlled with the 
JVM flag -Djna.tmpdir=<path>. 

As the native library is mapped into the JVM virtual address space as executable, 
the underlying mount point of the location that this code is extracted to must not be mounted with noexec 
as this prevents the JVM process from being able to map this code as executable. 

On some hardened Linux installations this is a default mount option for /tmp. 
One indication that the underlying mount is mounted with noexec is that at startup JNA 
will fail to load with a java.lang.UnsatisfiedLinkerError exception with a message along 
the lines of failed to map segment from shared object. 

Note that the exception message can differ amongst JVM versions. 
Additionally, the components of Elasticsearch that rely on execution of native code via JNA will fail with 
messages indicating that it is because JNA is not available. 

If you are seeing such error messages, you must remount the temporary directory used for JNA to not be mounted with noexec.

------------------
Note** on Bootstrap Checks

These bootstrap checks inspect a variety of Elasticsearch and system settings and compare them to values 
that are safe for the operation of Elasticsearch. If Elasticsearch is in development mode, 
any bootstrap checks that fail appear as warnings in the Elasticsearch log. 
If Elasticsearch is in production mode, any bootstrap checks that fail will cause Elasticsearch to refuse to start.


