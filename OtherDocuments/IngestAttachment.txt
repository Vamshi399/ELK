root@e3:~# /usr/share/elasticsearch/bin/elasticsearch-plugin install ingest-attachment
-> Downloading ingest-attachment from elastic
[=================================================] 100%   
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@     WARNING: plugin requires additional permissions     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
* java.lang.RuntimePermission accessClassInPackage.sun.java2d.cmm.kcms
* java.lang.RuntimePermission accessDeclaredMembers
* java.lang.RuntimePermission getClassLoader
* java.lang.reflect.ReflectPermission suppressAccessChecks
* java.security.SecurityPermission createAccessControlContext
* java.security.SecurityPermission insertProvider
* java.security.SecurityPermission putProviderProperty.BC
See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html
for descriptions of what these permissions allow and the associated risks.

Continue with installation? [y/N]y
-> Installed ingest-attachment
=====
root@e3:~# systemctl restart elasticsearch
root@e3:~# curl
curl: try 'curl --help' or 'curl --manual' for more information
root@e3:~# apt-get install curl
Reading package lists... Done
Building dependency tree       
Reading state information... Done
curl is already the newest version (7.58.0-2ubuntu3.8).
0 upgraded, 0 newly installed, 0 to remove and 107 not upgraded.
root@e3:~# curl 192.168.56.106:9200/_cat/plugins
data2 ingest-attachment 6.8.5

#if cluster setup,install the plugin on each machine & restart elasticsearch service

In order to use the attachment plugin, a pipeline must be used to process base64-encoded data in the field of a document. An ingest pipeline is a way of performing additional steps when indexing a document in Elasticsearch. While Elasticsearch comes pre-installed with some pipeline processors (which can perform actions such as removing or adding fields), the attachment plugin installs an additional processor that can be used when defining a pipeline.

1.	Create a pipeline called doc-parser which takes data from a field called encoded_doc and executes the attachment processor on the field:

PUT /_ingest/pipeline/doc-parser
{
  "description" : "Extract text from base-64 encoded documents",
  "processors" : [ { "attachment" : { "field" : "encoded_doc" } } ]
}

The doc-parser pipeline can now be specified when indexing documents to extract data from the encoded_doc field.
Note
By default, the attachment processor will create a new field called attachment with the parsed content of the target field.

use a base64 converter to convert a test line

2.
Add this to the test index, using the ?pipeline=doc_parser parameter to specify the new pipeline:
PUT /test/doc/rtf?pipeline=doc-parser
{
  "encoded_doc": "SGVsbG8gdGhpcyBpcyBtZQ=="
}


3. Test querying your Index
POST /_search
{
  "query": {
    "terms": {
      "attachment.content": ["hello"]
    }
  }
}
================================

Using FSCrawler
https://repo1.maven.org/maven2/fr/pilato/elasticsearch/crawler/fscrawler/2.5/fscrawler-2.5.zip
https://fscrawler.readthedocs.io/en/latest/user/getting_started.html

wget the zip
unzip
run the job for first time

root@e3:~/fscrawler-2.5# bin/fscrawler --config_dir ./Test data_check

root@e3:~/fscrawler-2.5# bin/fscrawler --config_dir ./Test data_check
This creates a Test folder and settings.json which needs to be updated

root@e3:~# ls fscrawler-2.5/Test/data_check/
_settings.json


--update path to pickup files that need to be indexed
--update ES path

root@e3:~/fscrawler-2.5# bin/fscrawler --config_dir ./Test data_check --loop 1
06:21:15,903 INFO  [f.p.e.c.f.FsCrawlerImpl] Starting FS crawler
06:21:15,977 WARN  [o.e.c.RestClient] request [PUT http://192.168.56.104:9200/data_check] returned 1 warnings: [299 Elasticsearch-6.8.5-78990e9 "[types removal] The parameter include_type_name should be explicitly specified in create index requests to prepare for 7.0. In 7.0 include_type_name will default to 'false', and requests are expected to omit the type name in mapping definitions."]
06:21:16,007 WARN  [o.e.c.RestClient] request [PUT http://192.168.56.104:9200/data_check_folder] returned 1 warnings: [299 Elasticsearch-6.8.5-78990e9 "[types removal] The parameter include_type_name should be explicitly specified in create index requests to prepare for 7.0. In 7.0 include_type_name will default to 'false', and requests are expected to omit the type name in mapping definitions."]
06:21:16,034 INFO  [f.p.e.c.f.FsParser] FS crawler started for [data_check] for [/tmp/DataContent] every [15m]
06:21:16,386 INFO  [f.p.e.c.f.FsParser] FS crawler is stopping after 1 run
06:21:16,455 INFO  [f.p.e.c.f.FsCrawlerImpl] FS crawler [data_check] stopped
06:21:16,465 INFO  [f.p.e.c.f.FsCrawlerImpl] FS crawler [data_check] stopped

GET /_cat/indices
GET /data_check/_search
GET /data_check/_mapping
GET /data_check/_search
{"_source": "{path}",
"query":{
  "match_phrase": {
    "content": "The aim of risk"
  }
}}
